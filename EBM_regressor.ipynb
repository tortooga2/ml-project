{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d417c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from fileinput import filename\n",
    "import random\n",
    "import math\n",
    "import matplotlib\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import sklearn.linear_model as ln\n",
    "import sklearn.ensemble as es\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBClassifier\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from interpret import show\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from cross_validate import cross_validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "375878c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def load_hotel_dataset_for_ebm(\n",
    "    csv_path: str,\n",
    "    selected_features: List[str] # <--- NEW PARAMETER\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Load the hotel dataset for ExplainableBoostingRegressor (EBM),\n",
    "    keeping only a specified list of valuable features.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str)         : Path to the CSV file.\n",
    "        selected_features (list): List of column names to INCLUDE in the final feature set X.\n",
    "\n",
    "    Returns:\n",
    "        X_df (pd.DataFrame)    : DataFrame with numeric + categorical cols.\n",
    "        y (np.ndarray)         : Reviewer_Score target array.\n",
    "        feature_names (list)   : Final list of feature names used.\n",
    "        feature_types (list)   : Corresponding list of feature types ('continuous' or 'nominal').\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # --- Target & Initial Cleaning ---\n",
    "    if \"Reviewer_Score\" not in df.columns:\n",
    "        raise ValueError(\"Expected 'Reviewer_Score' column as target.\")\n",
    "    \n",
    "    # -------------------------\n",
    "    # Feature Engineering/Parsing (ALWAYS REQUIRED)\n",
    "    # -------------------------\n",
    "    # 1. Parse Review_Date → year/month/day\n",
    "    df[\"Review_Date\"] = pd.to_datetime(df[\"Review_Date\"], errors=\"coerce\")\n",
    "    df[\"Review_Year\"] = df[\"Review_Date\"].dt.year\n",
    "    df[\"Review_Month\"] = df[\"Review_Date\"].dt.month\n",
    "    df[\"Review_Day\"] = df[\"Review_Date\"].dt.day\n",
    "    df = df.drop(columns=[\"Review_Date\"], errors='ignore')\n",
    "\n",
    "    # 2. Parse \"days_since_review\"\n",
    "    df[\"days_since_review\"] = (\n",
    "        df[\"days_since_review\"].astype(str).str.extract(r\"(\\d+)\", expand=False)\n",
    "    )\n",
    "    df[\"days_since_review\"] = pd.to_numeric(df[\"days_since_review\"], errors=\"coerce\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Feature Definition (Define ALL potential features here)\n",
    "    # -------------------------\n",
    "\n",
    "    # Define the *potential* numeric and categorical columns\n",
    "    POTENTIAL_CATEGORICAL_COLS = [\n",
    "        \"Hotel_Name\",\n",
    "        \"Reviewer_Nationality\",\n",
    "    ]\n",
    "    \n",
    "    POTENTIAL_NUMERIC_COLS = [\n",
    "        \"Additional_Number_of_Scoring\",\n",
    "        \"Average_Score\",\n",
    "        \"Review_Total_Negative_Word_Counts\",\n",
    "        \"Total_Number_of_Reviews\",\n",
    "        \"Review_Total_Positive_Word_Counts\",\n",
    "        \"Total_Number_of_Reviews_Reviewer_Has_Given\",\n",
    "        \"days_since_review\",\n",
    "        \"lat\",\n",
    "        \"lng\",\n",
    "        \"Review_Year\",\n",
    "        \"Review_Month\",\n",
    "        \"Review_Day\",\n",
    "    ]\n",
    "    \n",
    "    # Text columns to drop (unless you process them separately)\n",
    "    TEXT_COLS_TO_DROP = [\n",
    "        \"Hotel_Address\", \n",
    "        \"Positive_Review\", \n",
    "        \"Negative_Review\", \n",
    "        \"Tags\"\n",
    "    ]\n",
    "    \n",
    "    # -------------------------\n",
    "    # APPLY SELECTION FILTER\n",
    "    # -------------------------\n",
    "    \n",
    "    # Filter potential lists to keep only the ones in selected_features\n",
    "    # and only if they exist in the DataFrame columns.\n",
    "    categorical_cols = [\n",
    "        c for c in POTENTIAL_CATEGORICAL_COLS \n",
    "        if c in selected_features and c in df.columns\n",
    "    ]\n",
    "    \n",
    "    numeric_cols = [\n",
    "        c for c in POTENTIAL_NUMERIC_COLS \n",
    "        if c in selected_features and c in df.columns\n",
    "    ]\n",
    "\n",
    "    # Drop any large text columns that are NOT in the selected list\n",
    "    cols_to_drop = [\n",
    "        c for c in TEXT_COLS_TO_DROP \n",
    "        if c in df.columns and c not in selected_features\n",
    "    ]\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # -------------------------\n",
    "    # Final Type Conversion and Cleanup\n",
    "    # -------------------------\n",
    "\n",
    "    # 1. Force numeric conversion for numeric_cols\n",
    "    if numeric_cols:\n",
    "        df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # 2. Convert categorical columns to the EBM-friendly 'category' dtype\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    # 3. Build final feature lists\n",
    "    feature_names = numeric_cols + categorical_cols\n",
    "    feature_types = (\n",
    "        [\"continuous\"] * len(numeric_cols) +\n",
    "        [\"nominal\"] * len(categorical_cols)\n",
    "    )\n",
    "\n",
    "    # 4. Drop rows with any missing data in the selected features or target\n",
    "    df_clean = df.dropna(subset=feature_names + [\"Reviewer_Score\"])\n",
    "\n",
    "    X_df = df_clean[feature_names]\n",
    "    y = df_clean[\"Reviewer_Score\"].astype(float).to_numpy()\n",
    "\n",
    "    return X_df, y, feature_names, feature_types\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Example Usage: How to call the new function\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Based on your EBM analysis, you decide these are your 'valuable' features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e21a4c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "X_df shape: (512470, 13)\n",
      "y shape: (512470,)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "VALUABLE_COLS = [\n",
    "    \"Average_Score\",\n",
    "    \"Review_Total_Positive_Word_Counts\",\n",
    "    \"Review_Total_Negative_Word_Counts\",\n",
    "    \"Reviewer_Nationality\",\n",
    "    \"days_since_review\",\n",
    "    \"Total_Number_of_Reviews\",\n",
    "    \"Total_Number_of_Reviews_Reviewer_Has_Given\",\n",
    "    \"Review_Year\",\n",
    "    \"Review_Month\",\n",
    "    \"Review_Day\",\n",
    "    \"lat\",\n",
    "    \"lng\",\n",
    "    \"Hotel_Name\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "X_df, y, feature_names, feature_types = load_hotel_dataset_for_ebm(\"datasets/Hotel_Reviews.csv\", VALUABLE_COLS)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"X_df shape:\", X_df.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Seperate the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab22f10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Final EBM Model ===\n",
      "Trained final EBM model with best hyperparameters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/139818560644816/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/139818560644816/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals and Absolute Errors:\n",
      "Mean Residuals: -0.0004443875378803506\n",
      "Std Residuals: 1.2214984102554023\n",
      "Mean Absolute Errors: 0.9073622272242836\n",
      "Std Absolute Errors: 0.8177727999532304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# param_grid_ebc = {\n",
    "#     # \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "#     # \"max_bins\": [32, 64, 128],\n",
    "#     # \"interactions\": [1, 2, 5, 10]\n",
    "#     \"max_bins\" : [32, 64, 128],\n",
    "#     \"interactions\": [1, 2, 5, 10],\n",
    "#     \"interactions\": [0],\n",
    "#     \"learning_rate\": [0.02],\n",
    "#     \"max_rounds\": [50]\n",
    "# }\n",
    "\n",
    "# cv_results, best_params_ebc = cross_validate_model(\n",
    "#     ExplainableBoostingRegressor(),\n",
    "#     {\"feat\": feat, \"gnd\": gnd},\n",
    "#     param_grid_ebc,\n",
    "#     5,\n",
    "# )\n",
    "\n",
    "# cv_results.to_csv(\"ebm_cross_validation_results.csv\", index=False)\n",
    "# print(\"Saved CV results → ebm_cross_validation_results.csv\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Train Final Model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "print(\"\\n=== Training Final EBM Model ===\")\n",
    "best_model = ExplainableBoostingRegressor(\n",
    "    feature_names=feature_names,\n",
    "    learning_rate= 0.01,\n",
    "    max_bins=512,\n",
    "    interactions=66,\n",
    "    max_rounds=500,\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Trained final EBM model with best hyperparameters.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Save Feature Importances\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "show(best_model.explain_global())\n",
    "\n",
    "# feature_importance_df.to_csv(\"ebm_feature_importances.csv\", index=False)\n",
    "# print(\"Saved feature importances → ebm_feature_importances.csv\")\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 4. Predict on Test Set + Error Analysis\n",
    "# # ---------------------------------------------------------\n",
    "\n",
    "test_pred = best_model.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "# # residuals\n",
    "residuals = y_true - test_pred\n",
    "abs_error = np.abs(residuals)\n",
    "print(\"Residuals and Absolute Errors:\")\n",
    "np.mean(residuals)\n",
    "np.std(residuals)\n",
    "np.mean(abs_error)\n",
    "np.std(abs_error)\n",
    "print(\"Mean Residuals:\", np.mean(residuals))\n",
    "print(\"Std Residuals:\", np.std(residuals))\n",
    "print(\"Mean Absolute Errors:\", np.mean(abs_error))\n",
    "print(\"Std Absolute Errors:\", np.std(abs_error))\n",
    "\n",
    "# test_results_df = pd.DataFrame({\n",
    "#     \"True_Value\": y_true,\n",
    "#     \"Predicted_Value\": test_pred,\n",
    "#     \"Residual\": residuals,\n",
    "#     \"Absolute_Error\": abs_error,\n",
    "# })\n",
    "\n",
    "# test_results_df.to_csv(\"ebm_test_predictions_with_errors.csv\", index=False)\n",
    "# print(\"Saved test set predictions + errors → ebm_test_predictions_with_errors.csv\")\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 5. Compute & Export Summary Metrics\n",
    "# # ---------------------------------------------------------\n",
    "\n",
    "# mae = mean_absolute_error(y_true, test_pred)\n",
    "# mse = mean_squared_error(y_true, test_pred)\n",
    "# rmse = np.sqrt(mse)\n",
    "# r2 = r2_score(y_true, test_pred)\n",
    "\n",
    "# metrics_df = pd.DataFrame([{\n",
    "#     \"MAE\": mae,\n",
    "#     \"MSE\": mse,\n",
    "#     \"RMSE\": rmse,\n",
    "#     \"R2\": r2\n",
    "# }])\n",
    "\n",
    "# metrics_df.to_csv(\"ebm_test_metrics.csv\", index=False)\n",
    "# print(\"Saved summary test metrics → ebm_test_metrics.csv\")\n",
    "\n",
    "# # Also print them for quick reference\n",
    "# print(\"\\n=== Test Metrics ===\")\n",
    "# print(f\"MAE : {mae:.4f}\")\n",
    "# print(f\"RMSE: {rmse:.4f}\")\n",
    "# print(f\"R²  : {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
