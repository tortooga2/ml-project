{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b983e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from fileinput import filename\n",
    "import random\n",
    "import math\n",
    "import matplotlib\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import sklearn.linear_model as ln\n",
    "import sklearn.ensemble as es\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBClassifier\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from interpret import show\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from cross_validate import cross_validate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1cdff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (1496855, 68)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n"
     ]
    }
   ],
   "source": [
    "# Load only the numerical sensor data (assuming columns 4 onward are channels)\n",
    "sensor_data = np.loadtxt(\n",
    "    './datasets/CoST.csv',\n",
    "    delimiter=',',\n",
    "    skiprows=1,           # Skip the header row\n",
    ")\n",
    "print(f\"Loaded shape: {sensor_data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "feat = sensor_data[:, 4:].astype(float)  # All rows, columns from index 4 onward\n",
    "gnd_variant = sensor_data[:, 1].astype(int) - 1  # All rows, second column\n",
    "gnd_gesture = sensor_data[:, 2].astype(int) - 1  # All rows, third column\n",
    "\n",
    "print(np.unique(gnd_gesture))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat, gnd_gesture.astype(int), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb39eae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Final EBM Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/interpret/glassbox/_ebm/_ebm.py:1250: UserWarning:\n",
      "\n",
      "For multiclass we cannot currently visualize pairs and they will be stripped from the global explanations. Set interactions=0 to generate a fully interpretable glassbox model.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/joblib/externals/loky/backend/resource_tracker.py\"\u001b[0m, line \u001b[35m326\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mregistry[rtype]\u001b[0m\u001b[1;31m[name]\u001b[0m -= 1\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "\u001b[1;35mKeyError\u001b[0m: \u001b[35m'/dev/shm/joblib_memmapping_folder_2147395_6a0383ec3814413db134a62d9d5b43ae_e9bdf06a55fa471e9c31803017c33756/2147395-140047201050768-8ebc1c234e8146ad8304fb72a551d5a1.pkl'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained final EBM model with best hyperparameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/interpret/glassbox/_ebm/_ebm.py:2204: UserWarning:\n",
      "\n",
      "Dropping term feature_0017 & feature_0041 from explanation since we can't graph multinomial interactions.\n",
      "\n",
      "/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/interpret/glassbox/_ebm/_ebm.py:2204: UserWarning:\n",
      "\n",
      "Dropping term feature_0020 & feature_0035 from explanation since we can't graph multinomial interactions.\n",
      "\n",
      "/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/interpret/glassbox/_ebm/_ebm.py:2204: UserWarning:\n",
      "\n",
      "Dropping term feature_0027 & feature_0028 from explanation since we can't graph multinomial interactions.\n",
      "\n",
      "/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/interpret/glassbox/_ebm/_ebm.py:2204: UserWarning:\n",
      "\n",
      "Dropping term feature_0028 & feature_0035 from explanation since we can't graph multinomial interactions.\n",
      "\n",
      "/home/chasen/Projects/ml-project/.venv/lib/python3.13/site-packages/interpret/glassbox/_ebm/_ebm.py:2204: UserWarning:\n",
      "\n",
      "Dropping term feature_0029 & feature_0045 from explanation since we can't graph multinomial interactions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7916/140047196799312/ -->\n",
       "<iframe src=\"http://127.0.0.1:7916/140047196799312/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions:\n",
      "[12  6  5  2 13  2  0  4  8 13]\n",
      "True Values:\n",
      "[12  6  5  2 13  2  0 10  2  5]\n",
      "Accuracy: 0.4581405680576943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n=== Training Final EBM Model ===\")\n",
    "best_model = ExplainableBoostingClassifier(    \n",
    "    max_bins=64,              # down from 256/1024\n",
    "    max_interaction_bins=16,  # lighter interaction binning\n",
    "    interactions=5,           # or 5 if you really want some interactions\n",
    "    outer_bags=4,             # fewer bagged models\n",
    "    inner_bags=0,\n",
    "    max_rounds=1000,           # not 5000+\n",
    "    learning_rate=0.2,       # a bit higher to compensate fewer rounds\n",
    "    validation_size=0.1,      # keep early stopping\n",
    "    early_stopping_rounds=25,\n",
    "    n_jobs=-1,                # use all cores\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Trained final EBM model with best hyperparameters.\")\n",
    "\n",
    "\n",
    "show(best_model.explain_global())\n",
    "\n",
    "\n",
    "test_pred = best_model.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "print(\"Test Predictions:\")\n",
    "print(test_pred[:10])\n",
    "print(\"True Values:\")\n",
    "print(y_true[:10])\n",
    "\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = np.mean(test_pred == y_true)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "896e568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CoST shape: (1496855, 68)\n",
      "Max frame length: 1747\n",
      "Number of sequence starts: 7805\n",
      "Number of sequences after split: 7805\n",
      "Final tensor shape: R=7805, T_max=1747, C=64\n",
      "Final feature array shape: (7805, 640)\n",
      "Train size: 6244, Test size: 1561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load only the numerical sensor data (assuming columns 4 onward are channels)\n",
    "sensor_data = np.loadtxt(\n",
    "    \"./datasets/CoST.csv\",\n",
    "    delimiter=\",\",\n",
    "    skiprows=1,  # Skip the header row\n",
    ")\n",
    "print(f\"Loaded CoST shape: {sensor_data.shape}\")\n",
    "\n",
    "# Columns:\n",
    "# 0: subject\n",
    "# 1: variant\n",
    "# 2: gesture (class label)\n",
    "# 3: frame index\n",
    "# 4-67: ch1..ch64\n",
    "\n",
    "feat = sensor_data[:, 4:]                      # (N_frames, 64)\n",
    "gnd_variant = sensor_data[:, 1].astype(int)    # (N_frames,)\n",
    "gnd_gesture = sensor_data[:, 2].astype(int)    # (N_frames,)\n",
    "\n",
    "frame_column = sensor_data[:, 3].astype(int)   # (N_frames,) frame numbers\n",
    "\n",
    "max_frame = np.max(frame_column)\n",
    "print(f\"Max frame length: {max_frame}\")\n",
    "\n",
    "# ----- Build sequences based on frame == 1 -----\n",
    "sequence_change = []\n",
    "for i, frame in enumerate(frame_column):\n",
    "    if frame == 1:\n",
    "        sequence_change.append(i)\n",
    "\n",
    "print(f\"Number of sequence starts: {len(sequence_change)}\")\n",
    "\n",
    "tensor_feat = np.split(feat, sequence_change[1:])\n",
    "gnd_gesture_seq = gnd_gesture[sequence_change]   # gesture label per sequence\n",
    "gnd_variant_seq = gnd_variant[sequence_change]   # variant label per sequence\n",
    "\n",
    "print(f\"Number of sequences after split: {len(tensor_feat)}\")\n",
    "\n",
    "# ----- Pad sequences to length max_frame -----\n",
    "final_3d_array = np.zeros((len(tensor_feat), max_frame, 64), dtype=np.float32)\n",
    "\n",
    "for i, seq in enumerate(tensor_feat):\n",
    "    seq_len = seq.shape[0]\n",
    "    final_3d_array[i, :seq_len, :] = seq\n",
    "\n",
    "feat_seq = final_3d_array              # shape: (R, T_max, 64)\n",
    "gnd_seq = (gnd_gesture_seq - 1).astype(int)   # zero-based labels\n",
    "\n",
    "R, T_max, C_channels = feat_seq.shape\n",
    "print(f\"Final tensor shape: R={R}, T_max={T_max}, C={C_channels}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Hand-crafted time-series features (per sequence)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "feat_mean = np.mean(feat_seq, axis=1)          # (R, 64)\n",
    "feat_std = np.std(feat_seq, axis=1)            # (R, 64)\n",
    "feat_max = np.max(feat_seq, axis=1)            # (R, 64)\n",
    "\n",
    "feat_diff = np.diff(feat_seq, axis=1)          # (R, T_max-1, 64)\n",
    "feat_mean_abs_diff = np.mean(np.abs(feat_diff), axis=1)  # (R, 64)\n",
    "\n",
    "feat_accel = np.diff(feat_seq, axis=1, n=2)    # (R, T_max-2, 64)\n",
    "feat_std_accel = np.std(feat_accel, axis=1)    # (R, 64)\n",
    "\n",
    "mid_point = T_max // 2\n",
    "feat_mean_half1 = np.mean(feat_seq[:, :mid_point, :], axis=1)  # (R, 64)\n",
    "feat_mean_half2 = np.mean(feat_seq[:, mid_point:, :], axis=1)  # (R, 64)\n",
    "\n",
    "feat_max_time = np.argmax(feat_seq, axis=1)    # (R, 64)\n",
    "signs = np.sign(feat_seq)\n",
    "sign_changes = np.diff(signs, axis=1) != 0\n",
    "feat_zcr = np.sum(sign_changes, axis=1) / (T_max - 1)         # (R, 64)\n",
    "\n",
    "feat_max_time_norm = feat_max_time / T_max                    # (R, 64)\n",
    "feat_energy = np.sum(np.square(feat_seq), axis=1)             # (R, 64)\n",
    "\n",
    "# Stack features horizontally\n",
    "X_features = np.hstack(\n",
    "    [\n",
    "        feat_mean,\n",
    "        feat_std,\n",
    "        feat_max,\n",
    "        feat_mean_abs_diff,\n",
    "        feat_std_accel,\n",
    "        feat_mean_half1,\n",
    "        feat_mean_half2,\n",
    "        feat_zcr,\n",
    "        feat_max_time_norm,\n",
    "        feat_energy,\n",
    "    ]\n",
    ")\n",
    "# ... (Previous feature extraction and splitting code remains the same) ...\n",
    "\n",
    "print(f\"Final feature array shape: {X_features.shape}\")  # (R, 640)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Train/test split + scaling\n",
    "# ---------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features,\n",
    "    gnd_seq,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=gnd_seq,\n",
    ")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01dbb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting single EBM (sanity check)...\n",
      "Test accuracy (single EBM, no CV): 0.642\n",
      "Running cross-validation EBM (Optimized)...\n",
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n",
      "[CV 1/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=2;, score=0.622 total time=   7.3s\n",
      "[CV 2/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=2;, score=0.608 total time=   7.4s\n",
      "[CV 3/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=2;, score=0.612 total time=   7.2s\n",
      "[CV 1/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=4;, score=0.620 total time=   7.5s\n",
      "[CV 2/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=4;, score=0.612 total time=   7.6s\n",
      "[CV 3/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=4;, score=0.614 total time=   7.4s\n",
      "[CV 1/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=8;, score=0.622 total time=   7.9s\n",
      "[CV 2/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=8;, score=0.610 total time=   7.9s\n",
      "[CV 3/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=50, outer_bags=8;, score=0.615 total time=   7.5s\n",
      "[CV 1/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=100, outer_bags=2;, score=0.637 total time=  10.9s\n",
      "[CV 2/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=100, outer_bags=2;, score=0.632 total time=  11.1s\n",
      "[CV 3/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=100, outer_bags=2;, score=0.627 total time=  10.8s\n",
      "[CV 1/3] END inner_bags=0, interactions=0, learning_rate=0.01, max_bins=16, max_rounds=100, outer_bags=4;, score=0.644 total time=  11.1s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Fast sanity check: single EBM fit\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# OPTIMIZED SETTINGS FOR SPEED\n",
    "ebm_fast = ExplainableBoostingClassifier(\n",
    "    max_rounds=50,       # Reduced slightly for speed\n",
    "    outer_bags=4,\n",
    "    inner_bags=0,\n",
    "    learning_rate=0.05,\n",
    "    max_bins=32,\n",
    "    interactions=0,      # <--- CRITICAL: Disables heavy pair-detection (Major speedup)\n",
    "    n_jobs=-1,           # <--- CRITICAL: Uses all CPU cores\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Fitting single EBM (sanity check)...\")\n",
    "ebm_fast.fit(X_train, y_train)\n",
    "y_pred = ebm_fast.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy (single EBM, no CV): {test_acc:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Cross-validation with lighter settings\n",
    "# ---------------------------------------------------\n",
    "\n",
    "param_grid_ebc = {\n",
    "    \"max_bins\": [16, 32, 64],\n",
    "    \"learning_rate\": [0.01, 0.05],\n",
    "    \"interactions\": [0, 5],  # Keep interactions at 0\n",
    "    \"max_rounds\": [50, 100, 500],\n",
    "    \"outer_bags\": [2, 4, 8],\n",
    "    \"inner_bags\": [0, 4, 8],\n",
    "}\n",
    "\n",
    "def cross_validate_model_light(model, train, param_grid, cv_folds):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_folds,\n",
    "        verbose=3,\n",
    "        n_jobs=1,  # Keep this 1, because EBM handles the parallelism internally now\n",
    "    )\n",
    "    grid_search.fit(train[\"feat\"], train[\"gnd\"])\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"[BEST] Params: {best_params} -> Average Accuracy: {best_score}\")\n",
    "    return pd.DataFrame(grid_search.cv_results_), best_params\n",
    "\n",
    "\n",
    "print(\"Running cross-validation EBM (Optimized)...\")\n",
    "df_cost, best_params_ebc = cross_validate_model_light(\n",
    "    ExplainableBoostingClassifier(\n",
    "        max_rounds=50,       # Keep rounds low for CV\n",
    "        outer_bags=4,\n",
    "        inner_bags=0,\n",
    "        learning_rate=0.05,\n",
    "        max_bins=32,\n",
    "        interactions=0,      # <--- CRITICAL FIX\n",
    "        n_jobs=-1,           # <--- Uses all cores\n",
    "        random_state=42,\n",
    "    ),\n",
    "    {\"feat\": X_train, \"gnd\": y_train},\n",
    "    param_grid_ebc,\n",
    "    cv_folds=3,  # 3 folds is faster than 5\n",
    ")\n",
    "\n",
    "print(\"Best params for ExplainableBoostingClassifier on CoST:\", best_params_ebc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ExplainableBoostingRegressor(\n",
    "    max_rounds=best_params_ebc.get(\"max_rounds\", 50),       # Keep rounds low for CV\n",
    "    outer_bags=best_params_ebc.get(\"outer_bags\", 4),\n",
    "    inner_bags=best_params_ebc.get(\"inner_bags\", 0),\n",
    "    learning_rate=best_params_ebc.get(\"learning_rate\", 0.05),\n",
    "    max_bins=best_params_ebc.get(\"max_bins\", 32),\n",
    "    interactions=best_params_ebc.get(\"interactions\", 0),      # <--- CRITICAL FIX\n",
    "    n_jobs=-1,           # <--- Uses all cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Trained final EBM model with best hyperparameters.\")\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy (final EBM): {test_acc:.3f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
